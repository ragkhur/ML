{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "import os\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "# Load training and test datasets\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "# EDA: Overview of the data\n",
    "print(\"Train Data Overview:\")\n",
    "print(train_data.info())\n",
    "print(\"Test Data Overview:\")\n",
    "print(test_data.info())\n",
    "\n",
    "print(\"\\nMissing Values in Train Data:\\n\", train_data.isnull().sum())\n",
    "print(\"\\nMissing Values in Test Data:\\n\", test_data.isnull().sum())\n",
    "\n",
    "# EDA: Target distribution\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(x='Default', data=train_data)\n",
    "plt.title(\"Distribution of Target Variable (Default)\")\n",
    "plt.show()\n",
    "\n",
    "# EDA: Correlation heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "tdt = train_data.drop\n",
    "numeric_data = train_data.select_dtypes(include=[np.number])\n",
    "corr = numeric_data.corr()\n",
    "\n",
    "# corr = train_data.corr()\n",
    "sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"coolwarm\", cbar=True)\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# EDA: Correlation heatmap (exclude non-numeric columns)\n",
    "def plot_correlation_heatmap(data, target_col):\n",
    "    numeric_data = data.select_dtypes(include=[np.number])  # Select only numeric columns\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    corr = numeric_data.corr()  # Compute correlation on numeric data only\n",
    "    sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"coolwarm\", cbar=True)\n",
    "    plt.title(\"Correlation Heatmap\")\n",
    "    plt.show()\n",
    "    if target_col in corr.columns:\n",
    "        print(f\"Correlations with {target_col}:\\n\", corr[target_col].sort_values(ascending=False))\n",
    "    else:\n",
    "        print(\"Target column not found in the correlation matrix.\")\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_data(data, target_col=None, drop_cols=[]):\n",
    "    data = data.drop(columns=drop_cols)  # Drop unnecessary columns\n",
    "    le = LabelEncoder()\n",
    "    \n",
    "    # Encoding binary categorical variables\n",
    "    binary_cols = ['HasMortgage', 'HasDependents', 'HasCoSigner']\n",
    "    for col in binary_cols:\n",
    "        data[col] = data[col].map({'Yes': 1, 'No': 0})\n",
    "    \n",
    "    # Encoding multi-class categorical variables\n",
    "    categorical_cols = ['Education', 'EmploymentType', 'MaritalStatus', 'LoanPurpose']\n",
    "    for col in categorical_cols:\n",
    "        data[col] = le.fit_transform(data[col])\n",
    "    \n",
    "    # Scaling numerical features\n",
    "    scaler = StandardScaler()\n",
    "    numerical_cols = ['Age', 'Income', 'LoanAmount', 'CreditScore', \n",
    "                      'MonthsEmployed', 'InterestRate', 'LoanTerm', 'DTIRatio', 'NumCreditLines']\n",
    "    data[numerical_cols] = scaler.fit_transform(data[numerical_cols])\n",
    "    \n",
    "    if target_col:\n",
    "        X = data.drop(columns=[target_col])\n",
    "        y = data[target_col]\n",
    "        return X, y\n",
    "    return data\n",
    "\n",
    "# Drop irrelevant columns and preprocess\n",
    "plot_correlation_heatmap(train_data, \"Default\")\n",
    "X_train, y_train = preprocess_data(train_data, target_col=\"Default\", drop_cols=[\"LoanID\"])\n",
    "X_test = preprocess_data(test_data, drop_cols=[\"LoanID\"])\n",
    "\n",
    "# Split training data into train and validation sets\n",
    "X_train_split, X_val, y_train_split, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"LightGBM\": LGBMClassifier(random_state=42)\n",
    "}\n",
    "\n",
    " # Train and evaluate models\n",
    "model_performance = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_split, y_train_split)\n",
    "    y_pred = model.predict(X_val)\n",
    "    model_performance[name] = accuracy_score(y_val, y_pred)\n",
    "    print(f\"{name} Classification Report:\\n{classification_report(y_val, y_pred)}\")\n",
    "\n",
    "# Hyperparameter tuning for Random Forest\n",
    "rf_params = {\n",
    "    \"n_estimators\": [50, 100, 200],\n",
    "    \"max_depth\": [None, 10, 20],\n",
    "    \"min_samples_split\": [2, 5, 10]\n",
    "}\n",
    "rf_grid = GridSearchCV(RandomForestClassifier(random_state=42), rf_params, cv=3, scoring=\"accuracy\")\n",
    "rf_grid.fit(X_train_split, y_train_split)\n",
    "best_rf = rf_grid.best_estimator_\n",
    "print(\"Best Random Forest Params:\", rf_grid.best_params_)\n",
    "model_performance[\"Tuned Random Forest\"] = accuracy_score(y_val, best_rf.predict(X_val))\n",
    "\n",
    "# Hyperparameter tuning for LightGBM\n",
    "lgbm_params = {\n",
    "    \"n_estimators\": [50, 100, 200],\n",
    "    \"learning_rate\": [0.01, 0.1, 0.2],\n",
    "    \"max_depth\": [-1, 10, 20]\n",
    "}\n",
    "lgbm_grid = GridSearchCV(LGBMClassifier(random_state=42), lgbm_params, cv=3, scoring=\"accuracy\")\n",
    "lgbm_grid.fit(X_train_split, y_train_split)\n",
    "best_lgbm = lgbm_grid.best_estimator_\n",
    "print(\"Best LightGBM Params:\", lgbm_grid.best_params_)\n",
    "model_performance[\"Tuned LightGBM\"] = accuracy_score(y_val, best_lgbm.predict(X_val))\n",
    "\n",
    "# Final predictions on test data\n",
    "test_predictions = {}\n",
    "for name, model in models.items():\n",
    "    test_predictions[name] = model.predict(X_test)\n",
    "\n",
    "test_predictions[\"Tuned Random Forest\"] = best_rf.predict(X_test)\n",
    "test_predictions[\"Tuned LightGBM\"] = best_lgbm.predict(X_test)\n",
    "\n",
    "# # Save predictions to CSV\n",
    "output_dir = \"./output_predictions/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "for name, preds in test_predictions.items():\n",
    "    output = pd.DataFrame({\"LoanID\": test_data[\"LoanID\"], \"Default\": preds})\n",
    "    file_path = f\"{output_dir}predictions_{name.replace(' ', '_')}.csv\"\n",
    "    output.to_csv(file_path, index=False)\n",
    "    print(f\"Saved: {file_path}\")\n",
    "    output.to_csv(f\"predictions_{name.replace(' ', '_')}.csv\", index=False)\n",
    "\n",
    "# # Print final model performance\n",
    "print(\"Model Performance:\", model_performance)\n",
    "\n",
    "\n",
    "\n",
    "# Train Neural Network using Keras\n",
    "def train_neural_network(X_train, y_train, X_val, y_val):\n",
    "    input_dim = X_train.shape[1]\n",
    "    model = Sequential([\n",
    "        Dense(64, input_dim=input_dim, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=20, batch_size=32, verbose=1)\n",
    "    return model\n",
    "\n",
    "# Train SVM\n",
    "def train_svm(X_train, y_train, X_val, y_val):\n",
    "    # svm_model = SVC(probability=True, kernel='rbf', random_state=42)\n",
    "    # svm_model.fit(X_train, y_train)\n",
    "    \n",
    "    pca = PCA(n_components=0.95)\n",
    "    X_train_reduced = pca.fit_transform(X_train)\n",
    "\n",
    "    model = SVC(kernel='linear')\n",
    "    model.fit(X_train_reduced, y_train)\n",
    "\n",
    "    y_pred = svm_model.predict(X_val)\n",
    "    print(\"SVM Classification Report:\\n\", classification_report(y_val, y_pred))\n",
    "    return svm_model\n",
    "\n",
    "# Train and evaluate Neural Network\n",
    "nn_model = train_neural_network(X_train_split, y_train_split, X_val, y_val)\n",
    "y_val_pred_nn = (nn_model.predict(X_val) > 0.5).astype(int).flatten()\n",
    "print(\"Neural Network Classification Report:\\n\", classification_report(y_val, y_val_pred_nn))\n",
    "model_performance[\"Neural Network\"] = accuracy_score(y_val, y_val_pred_nn)\n",
    "\n",
    "# Train and evaluate SVM\n",
    "svm_model = train_svm(X_train_split, y_train_split, X_val, y_val)\n",
    "model_performance[\"SVM\"] = accuracy_score(y_val, svm_model.predict(X_val))\n",
    "\n",
    "# Final predictions\n",
    "test_predictions[\"Neural Network\"] = (nn_model.predict(X_test) > 0.5).astype(int).flatten()\n",
    "test_predictions[\"SVM\"] = svm_model.predict(X_test)\n",
    "\n",
    "# Save predictions for Neural Network and SVM\n",
    "for name in [\"Neural Network\", \"SVM\"]:\n",
    "    output = pd.DataFrame({\"LoanID\": test_data[\"LoanID\"], \"Default\": test_predictions[name]})\n",
    "    file_path = f\"{output_dir}predictions_{name.replace(' ', '_')}.csv\"\n",
    "    output.to_csv(file_path, index=False)\n",
    "    print(f\"Saved: {file_path}\")\n",
    "\n",
    "# Final performance summary\n",
    "print(\"Model Performance:\", model_performance)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
